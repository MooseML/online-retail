{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2B_0heO7u8z"
   },
   "source": [
    "# Holiday Shopper Segmentation: Clustering, Frequent Itemset Mining, & Recommendations\n",
    "\n",
    "\n",
    "**Data Source:** [UC Irvine Machine Learning Repository: Online Retail Dataset](https://archive.ics.uci.edu/dataset/352/online+retail)  \n",
    "\n",
    "---\n",
    "## Approach and Methodology\n",
    "1. Problem Definition and Data Gathering\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Model Building and Analysis\n",
    "4. Discussion and Conclusion\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgA72rROR8rT"
   },
   "source": [
    "## 1. Problem Definition and Data Gathering\n",
    "\n",
    "## Project Topic\n",
    "\n",
    "To group holiday shoppers into meaningful segments using unsupervised learning techniques. Businesses that understand customer behavior, especially during peak shopping seasons, can adjust their marketing strategies more effectively. After exploring this e-commerce dataset and noticing that the majority of purchases occur in November and December, I decided to focus on holiday time period transactions. Because there are no explicit labels for holiday shopper type, clustering is a great approach to discover these segments.\n",
    "\n",
    "\n",
    "## Project Goal\n",
    "\n",
    "The primary goal is to apply **K-Means** and **Hierarchical Clustering** to segment holiday shoppers into distinct groups:\n",
    "\n",
    " - Non-Holiday Shoppers (low holiday purchases and spending)\n",
    " - Moderate Holiday Shoppers (moderate holiday purchases and spending)\n",
    " - Frequent Holiday Shoppers (high holiday purchases and spending)\n",
    "\n",
    "By identifying these clusters, this business can personalize promotions, recommend relevant products, and develop targeted marketing campaigns to boost sales. In addition to clustering, **Frequent Itemset Mining** (via FPGrowth) will discover which items often occur together, supporting a simple **rule-based recommendation system** that suggests products frequently purchased together.\n",
    "\n",
    "Finally, I will also show a more advanced **recommender using implicit feedback** (purchased vs. not purchased) with the implicit library. This dual approach of simple rule-based and implicit collaborative filtering recommenders highlights two different methods for making product suggestions to holiday shoppers.\n",
    "\n",
    "\n",
    "## Data Section\n",
    "\n",
    "### Data Source\n",
    "\n",
    "The data used in this project is sourced from the [UC Irvine Machine Learning Repository: Online Retail Dataset](https://archive.ics.uci.edu/dataset/352/online+retail). It is a publicly available dataset provided by a UK-based and registered online retail company. The data can be downloaded from the link above.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "The dataset contains 8 features and 541,909 rows of customer data, with a total size of  **23.7 MB**. Each row represents a transaction, and the columns represent attributes about the transaction. Transactions ocurred from December 1, 2010 to December 31, 2011.\n",
    "\n",
    "\n",
    "- **Number of rows (samples):** 541,909  \n",
    "- **Number of columns (features):** 8  \n",
    "- **Data Size:** ~23.7 MB\n",
    "\n",
    "**Key Features:**\n",
    "- **Categorical Features:** `InvoiceNo`, `StockCode`, `Description`,  `InvoiceDate`, `CustomerID`, `Country`\n",
    "- **Numerical Features:** `Quantity`, `UnitPrice`\n",
    "\n",
    "**Context:**  \n",
    "The retailer specializes in unique gifts and sold through catalogs and phone orders before transitioning online two years before this datasets creation. This dataset has a large number of transactions, primarily in the UK and Europe over a 13-month period, making it a good source for understanding customer behavior.\n",
    "\n",
    "**Why This Dataset?**\n",
    "\n",
    "With both categorical and numerical features, the Online Retail data provides a realistic setting for unsupervised analysis of shopper patterns. Also, it provides a real-world application for holiday shopper segmentation and product recommendation.\n",
    "\n",
    "**Data Attributes:**\n",
    "- Total rows: **541,909**  \n",
    "- Total columns: **8**  \n",
    "- Examples of column names: `InvoiceNo`, `StockCode`, `Description`, `InvoiceDate`, `CustomerID`, `Country`, `Quantity`, `UnitPrice`   \n",
    "\n",
    "In the following sections, I will conduct Exploratory Data Analysis (EDA) to clean and understand the data, then build and analyze the clustering models and recommender systems, concluding with discussions on how businesses could apply these findings to increase holiday sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kSdlSXX-VhI"
   },
   "source": [
    "##  Necessary Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# statistical and mathematical libraries\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (silhouette_score, calinski_harabasz_score, davies_bouldin_score)\n",
    "\n",
    "# frequent pattern mining\n",
    "from mlxtend.frequent_patterns import fpgrowth, apriori, association_rules\n",
    "\n",
    "# matrix and recommendation libraries\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "import optuna\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# set matplotlib inline (for Jupyter Notebooks)\n",
    "%matplotlib inline\n",
    "\n",
    "# deprecation warning supression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# OpenBLAS single thread operations\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "# similarity between two strings\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYCPk0X_-Xf5"
   },
   "source": [
    "## Loading the Data, Inpsecting, Handling Missing Values, and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Online Retail.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxLB7PQG-ZsQ"
   },
   "source": [
    "##  Data Cleaning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv6WvdsGj1Ed"
   },
   "source": [
    "Here I check how many NaN values are present in the CustomerID column and decide whether to drop them or replace them with \"Unknown.\" Since there are 24.93% missing values for CustomerID, I considered filling these with Unkown because dropping almost 25% of the data would significantly reduce the dataset's size. However, for the purpose of this project, having the CustomerID is important to provide targeted marketing to these customers. If no CustomerID is present, there is little identifiable information. Also, filling missing ID values with \"Unknown\" would introduce noise because rows with this value would cluster together without adding actionable insights. Therefore, I decided to drop these rows and saw little impact on the model performance. Additionally, since there are only 1454 rows missing descriptions (about 0.27%) I dropped these rows. Frequent itemset mining is clearer to see in action when you can tell what the actual items are with the description. Creating a returns dataframe for orders with negative quantity is a great consideration, especially to identify customers with frequent returns. However, in the scope of this project I want to focus on finding revenue generating insights and recommendations, so cancelled/returned orders were also removed. Lastly, to standardize the descriptions for the same stock codes, I imputed the most common description for a given stock code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print basic df info\n",
    "print(\"Initial DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# explore missing values\n",
    "missing_values = df.isna().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_report = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage}).sort_values(by='Percentage', ascending=False)\n",
    "print(missing_report)\n",
    "\n",
    "# handle missing CustomerIDs\n",
    "missing_customer_ids = df['CustomerID'].isna().sum()\n",
    "missing_percent = (missing_customer_ids / len(df)) * 100\n",
    "print(f\"Missing CustomerID values: {missing_customer_ids} ({missing_percent:.3f}%)\")\n",
    "\n",
    "if missing_percent > 25:\n",
    "    # fill with Unknown\n",
    "    df['CustomerID'] = df['CustomerID'].fillna('Unknown')\n",
    "    print(\"CustomerID filled with 'Unknown'\")\n",
    "else:\n",
    "    df = df.dropna(subset=['CustomerID'])\n",
    "    print(\"Dropped all rows with missing CustomerID\")\n",
    "\n",
    "# drop rows with missing descriptions\n",
    "df = df.dropna(subset=['Description'])\n",
    "print(\"Dropped all rows with missing descriptions\")\n",
    "\n",
    "# identify canceled orders (start with C and have negative quantity)\n",
    "canceled_orders = df[df['InvoiceNo'].str.startswith('C', na=False)]\n",
    "print(f\"Number of canceled orders dropped: {len(canceled_orders)}\")\n",
    "# drop canceled orders\n",
    "df.drop(canceled_orders.index, inplace=True)\n",
    "\n",
    "# check unique descriptions before mode imputation\n",
    "unique_descriptions_before = df.groupby('StockCode')['Description'].nunique()\n",
    "print(\"Unique Descriptions Before: \", len(unique_descriptions_before[unique_descriptions_before > 1]))\n",
    "\n",
    "# impute mode description\n",
    "description_mode_dict = (df.groupby('StockCode')['Description'].agg(lambda x: x.value_counts().index[0]) # pick most common description\n",
    "                         .to_dict())\n",
    "df['Description_mode'] = df['StockCode'].map(description_mode_dict)\n",
    "df['Description_mode'] = (df['Description_mode'] if 'Description_mode' in df.columns else df['Description'])\n",
    "df.info()\n",
    "\n",
    "#check unique descriptions after mode imputation\n",
    "unique_descriptions_after = df.groupby('StockCode')['Description_mode'].nunique()\n",
    "print(\"Unique Descriptions After: \", len(unique_descriptions_after[unique_descriptions_after > 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG7PfXnpFaDC"
   },
   "source": [
    "Even after mode imputation the are cases where descriptions for different StockCodes are essentially the same. For example, StockCode A: \"Hand Warmer Retro Spot\"/StockCode B: \"Hand Warmer Red Retro Spot\" I'm assuming these are the same product and mode imputation doesn't address this. So, I define a similarity function to calculate how similar two strings are. Then I define a function to standardize the product descriptions. It groups data by StockCode and identifies products with consistent pricing (minimal price variation). It replaces highly similar descriptions (similarity score ≥ 0.95) with the most frequent one to standardize naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity function\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def standardize_descriptions(df, column_name):\n",
    "    # group by StockCode and get unique descriptions\n",
    "    multiple_descriptions = df.groupby('StockCode')[column_name].unique()\n",
    "    # calc price stdev w/in each StockCode group\n",
    "    price_std_dev = df.groupby('StockCode')['UnitPrice'].std()\n",
    "    # price variation threshold (20% of mean price)\n",
    "    price_variation_threshold = 0.20\n",
    "    # high similarity mappings\n",
    "    description_mapping = {}\n",
    "    print(f\"Examples of high-similarity descriptions in {column_name}:\")\n",
    "\n",
    "    # counter to limit printed examples\n",
    "    example_count = 0\n",
    "    for stock_code, descriptions in multiple_descriptions.items():\n",
    "        std_dev = price_std_dev[stock_code]\n",
    "        mean_price = df[df['StockCode'] == stock_code]['UnitPrice'].mean()\n",
    "        price_threshold = mean_price * price_variation_threshold\n",
    "        # check if price variation is within the range\n",
    "        if std_dev <= price_threshold:\n",
    "            for i in range(len(descriptions)):\n",
    "                for j in range(i + 1, len(descriptions)):\n",
    "                    sim_score = similar(descriptions[i], descriptions[j])\n",
    "                    if sim_score >= 0.95: # high threshold for typos\n",
    "                        # map less frequent description to the most frequent\n",
    "                        description_mapping[descriptions[j]] = descriptions[i]\n",
    "                        if example_count < 10:\n",
    "                            print(\n",
    "                                f\"StockCode: {stock_code}, Description 1: '{descriptions[i]}', \"\n",
    "                                f\"Description 2: '{descriptions[j]}', Similarity: {sim_score:.2f}\")\n",
    "                            example_count += 1\n",
    "\n",
    "    # apply the mapping\n",
    "    df[column_name] = df[column_name].replace(description_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply standardization to 'Description'\n",
    "df = standardize_descriptions(df, column_name='Description')\n",
    "# apply standardization to 'Description_mode'\n",
    "df = standardize_descriptions(df, column_name='Description_mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m82h6go-FfVX"
   },
   "source": [
    "Checking ambiguous cases:\n",
    "\n",
    "Some descriptions still fall into a similarity range (.85-.95) that suggests they might refer to the same product but need manual review. These ambiguous cases are not addressed by the mode imputation because they might occur after missing values are filled. Here, I identify cases where descriptions have a moderate similarity score and store them in a list for manual review, helping spot less obvious naming inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ambiguous_cases(df, column_name):\n",
    "    ambiguous_cases = []\n",
    "    multiple_descriptions = df.groupby('StockCode')[column_name].unique()\n",
    "    for stock_code, descriptions in multiple_descriptions.items():\n",
    "        for i in range(len(descriptions)):\n",
    "            for j in range(i + 1, len(descriptions)):\n",
    "                sim_score = similar(descriptions[i], descriptions[j])\n",
    "                if 0.85 <= sim_score < 0.95:\n",
    "                    ambiguous_cases.append((stock_code, descriptions[i], descriptions[j], sim_score))\n",
    "\n",
    "    # output ambiguous cases for review\n",
    "    print(f\"Number of ambiguous cases in {column_name}: {len(ambiguous_cases)}\")\n",
    "    return ambiguous_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_cases_description = find_ambiguous_cases(df, column_name='Description')\n",
    "ambiguous_cases_description_mode = find_ambiguous_cases(df, column_name='Description_mode')\n",
    "print(\"Ambiguous cases for 'Description':\", ambiguous_cases_description)\n",
    "print(\"Ambiguous cases for 'Description_mode':\", ambiguous_cases_description_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMH31XNp8Dk6"
   },
   "source": [
    "This cell manually maps specific descriptions to standardized forms and appends a unique suffix to StockCode for products with distinct descriptions. Remaining duplicates are assigned unique identifiers using a group numbering strategy. The cell below checks for remaining duplicates in StockCode and Description after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mappings for same products\n",
    "description_mapping = {\n",
    "    \"FLOWER FAIRY,5 SUMMER B'DRAW LINERS\": \"FLOWER FAIRY 5 SUMMER DRAWER LINERS\",\n",
    "    \"FLOWER FAIRY 5 DRAWER LINERS\": \"FLOWER FAIRY 5 SUMMER DRAWER LINERS\",\n",
    "    \"SET/5 RED SPOTTY LID GLASS BOWLS\": \"SET/5 RED RETROSPOT LID GLASS BOWLS\",\n",
    "    \"GIN + TONIC DIET METAL SIGN\": \"GIN AND TONIC DIET METAL SIGN\",\n",
    "    \"PARTY CONES CANDY DECORATION\": \"PARTY CONES CANDY TREE DECORATION\",\n",
    "    \"DECORATION SITTING BUNNY\": \"EASTER DECORATION SITTING BUNNY\",\n",
    "    \"DECORATION , WOBBLY RABBIT , METAL \": \"DECORATION WOBBLY RABBIT METAL\",\n",
    "    \"SQUARECUSHION COVER PINK UNION JACK\": \"SQUARECUSHION COVER PINK UNION FLAG\",\n",
    "    \"PAPER LANTERN 5 POINT STUDDED STAR\": \"PAPER LANTERN 5 POINT SEQUIN STAR\",\n",
    "    \"LARGE JEWELLERY STAND\": \"LARGE DECO JEWELLERY STAND\",\n",
    "    \"SMALL JEWELLERY STAND\": \"SMALL DECO JEWELLERY STAND\",\n",
    "    \"DOLLCRAFT GIRL AMELIE\": \"DOLLCRAFT GIRL AMELIE KIT\",\n",
    "    \"MISELTOE HEART WREATH CREAM\": \"MISTLETOE HEART WREATH CREAM\",\n",
    "    \"CLASSIC SUGAR DISPENSER\": \"CLASSIC CAFE SUGAR DISPENSER\",\n",
    "    \"RETO LEAVES MAGNETIC SHOPPING LIST\": \"LEAVES MAGNETIC  SHOPPING LIST\",\n",
    "    \"JINGLE BELL HEART ANTIQUE GOLD\": \"BELL HEART ANTIQUE GOLD\",\n",
    "    \"WRAP VINTAGE LEAF DESIGN\": \"WRAP VINTAGE PETALS  DESIGN\",\n",
    "    \"SET OF 4 KNICK KNACK TINS LEAF\": \"SET OF 4 KNICK KNACK TINS LEAVES \",\n",
    "    \"STORAGE TIN VINTAGE LEAF\": \"ROUND STORAGE TIN VINTAGE LEAF\",\n",
    "    \"KIDS CUTLERY DOLLY GIRL \": \"CHILDRENS CUTLERY DOLLY GIRL\",\n",
    "    \"KIDS CUTLERY SPACEBOY \": \"CHILDRENS CUTLERY SPACEBOY \",\n",
    "    \"FOOD COVER WITH BEADS , SET 2 SIZES\": \"FOOD COVER WITH BEADS SET 2 \",\n",
    "    \"MINT DINER CLOCK\": \"MINT DINER WALL CLOCK\",\n",
    "    \"SET 12 COLOUR PENCILS DOILEY\": \"SET 12 COLOURING PENCILS DOILY\",\n",
    "    \"SET 12 COLOURING PENCILS DOILEY\": \"SET 12 COLOURING PENCILS DOILY\",\n",
    "    \"SET 36 COLOURING PENCILS DOILEY\": \"SET 36 COLOUR PENCILS DOILEY\",\n",
    "    \"DECROTIVEVINTAGE COFFEE GRINDER BOX\": \"VINTAGE COFFEE GRINDER BOX\",\n",
    "    \"WALL ART BICYCLE SAFETY\": \"WALL ART BICYCLE SAFTEY \",\n",
    "    \"SMOKEY GREY COLOUR GLASS\": \"SMOKEY GREY COLOUR D.O.F. GLASS\",\n",
    "    \"FLOWER PURPLE CLOCK W/SUCKER\": \"FLOWER PURPLE CLOCK WITH SUCKER\",\n",
    "    \"HEN HOUSE W CHICK STANDING\": \"HEN HOUSE WITH CHICK STANDING\",\n",
    "    \"S/4 VALENTINE DECOUPAGE HEART BOX\": \"SET 4 VALENTINE DECOUPAGE HEART BOX\",\n",
    "    \"SILVER M.O.P. ORBIT NECKLACE\": \"SILVER/MOP ORBIT NECKLACE\",\n",
    "    \"SILVER/BLACK ORBIT NECKLACE\": \"SILVER AND BLACK ORBIT NECKLACE\"}\n",
    "\n",
    "# apply description mapping\n",
    "df['Description'] = df['Description'].replace(description_mapping)\n",
    "\n",
    "# list of stock codes with distinct descriptions\n",
    "distinct_stock_codes = ['21232', '21928', '22134', '22135', '22837', '22939', '22952', '23035', '23103', '23106', '23107', '23126', '23131', '84828', '85123A', '85185B']\n",
    "\n",
    "# append unique suffix for distinct descriptions\n",
    "df['StockCode'] = df.apply(\n",
    "    lambda row: row['StockCode'] + \"_\" + row['Description'].replace(\" \", \"_\")[:5]\n",
    "    if row['StockCode'] in distinct_stock_codes else row['StockCode'],\n",
    "    axis=1)\n",
    "# append unique identifier for remaining duplicates\n",
    "df['StockCode'] = df.groupby(['StockCode', 'Description']).ngroup().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast StockCode as a string and check for remaining duplicates\n",
    "df['StockCode'] = df['StockCode'].astype(str)\n",
    "remaining_duplicates = df.groupby('StockCode')['Description'].nunique()\n",
    "print(remaining_duplicates[remaining_duplicates > 1]) # output no duplicates\n",
    "# debug remaining duplicates\n",
    "problematic_stock_codes = remaining_duplicates[remaining_duplicates > 1].index.tolist()\n",
    "df_problematic = df[df['StockCode'].isin(problematic_stock_codes)].sort_values('StockCode')\n",
    "print(df_problematic[['StockCode', 'Description', 'UnitPrice']])\n",
    "print(len(df_problematic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shwKuIoIgSDe"
   },
   "source": [
    "## Basic Feature Creation for Further Inspection\n",
    "\n",
    "In this section, I extract date features for my EDA section. I was interested in looking at purchases across different time periods, with a particular focus on holiday purchases. `InvoiceDate` needed to be converted to datetime format for easier manipulation. A date is not inherently categorical, but it will be used in this code to derive other features like (day of the week, month, quater, and holiday period). I chose the period from 11/14 to the end of December to encompass as many holidays as I could think of: Black Friday, Christmas, Hanukkah, Kwanzaa, Boxing Day, Las Posadas, Saint Nicholas Day, Bodhi Day, and Saint Lucia Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change InvoiceDate to datetime\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# extract date features\n",
    "df.loc[:, 'InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "df.loc[:, 'Year'] = df['InvoiceDate'].dt.year.astype('category')\n",
    "df.loc[:, 'Month'] = df['InvoiceDate'].dt.month_name().astype('category')\n",
    "df.loc[:, 'Quarter'] = df['InvoiceDate'].dt.to_period('Q').astype('category')\n",
    "df.loc[:, 'DayOfWeek'] = df['InvoiceDate'].dt.day_name().astype('category')\n",
    "\n",
    "def holiday_period(date):\n",
    "    if date.month == 11 and date.day >= 14:\n",
    "        return 'Black Friday'\n",
    "    elif date.month == 12:\n",
    "        return 'Holiday Season'\n",
    "    else:\n",
    "        return 'Non-Holiday'\n",
    "\n",
    "df['HolidayPeriod'] = df['InvoiceDate'].apply(holiday_period).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtEZ9A9OhoAp"
   },
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fax_YBSxkwgm"
   },
   "source": [
    "## Purchase Frequency/Customer Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akcQ0WJJlayj"
   },
   "source": [
    "As can be seen in the first subplot, the data is highly skewed due to a few customers that have very high purchase frequency. To handle the skewness I will try two techniques. First, I will cap the maximum purchase frequency, then I will plot the distribution on a log scale to compress extreme values and reveal trends for the majority of customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_purchase_counts = df['CustomerID'].value_counts()\n",
    "\n",
    "# create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 6)) # 2 rows, 2 cols\n",
    "\n",
    "# 1. og distribution\n",
    "sns.histplot(customer_purchase_counts, bins=50, kde=False, ax=axes[0])\n",
    "axes[0].set_title('Purchase Frequency (Original)')\n",
    "axes[0].set_xlabel('Number of Purchases')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 2. capped distribution\n",
    "capped_counts = customer_purchase_counts.copy()\n",
    "capped_counts[capped_counts > 1000] = 1000\n",
    "sns.histplot(capped_counts, bins=30, kde=False, ax=axes[1])\n",
    "axes[1].set_title('Capped Purchase Frequency (max=1000)')\n",
    "axes[1].set_xlabel('Number of Purchases (Capped)')\n",
    "axes[1].set_ylabel('Number of Customers')\n",
    "\n",
    "# 3. log-transformed distribution\n",
    "log_counts = customer_purchase_counts[customer_purchase_counts > 0].apply(lambda x: np.log10(x))\n",
    "sns.histplot(log_counts, bins=30, kde=False, ax=axes[2])\n",
    "axes[2].set_title('Log-Transformed Frequency')\n",
    "axes[2].set_xlabel('Log10(Number of Purchases)')\n",
    "axes[2].set_ylabel('Number of Customers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc6y0tHTmDhA"
   },
   "source": [
    "## Insights:\n",
    "1. Capped Distribution of Purchase Frequencies shows that most customers make few purchases, with a large drop off past the 0-50 range. Most customers are one time buyers, which makes sense for retail purchases. There are a few frequent buyers, which should be explored because they could be important targets for marketing and advertising.\n",
    "2. Log-Transformed Distribution of Purchase Frequencies shows a mostly normal distribution, meaning that most customers are in a moderate range of purchase frequencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MIiuKVTlAt4"
   },
   "source": [
    "## Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique countries:\", df['Country'].nunique())\n",
    "print(\"Top 10 countries by transaction count:\")\n",
    "print(df['Country'].value_counts().head(10))\n",
    "\n",
    "# bar plot\n",
    "df['Country'].value_counts().head(10).plot(kind='bar', figsize=(8,5))\n",
    "plt.title('Top 10 Countries by Transaction Count')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9ECKbKZmH_-"
   },
   "source": [
    "## Insights\n",
    "The United Kingdom has an overwhelming majority of transactions compared to other countries, meaning that it is the primary market for this dataset. Germany, France, and EIRE (Ireland) have a noticeable number of transactions but are way lower than the UK. The remaining countries have few transaction counts, which suggests limited business activity or customer engagement in those regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_Ni5F2SlGzi"
   },
   "source": [
    "## Product-Level Analysis (Quantity, UnitPrice, Amount)\n",
    "\n",
    "Here, I create an Amount feature by multilpying the quantity by the unit price. This will help clustering by providing an additional feature defining the total amount a customer spent. Then, I plotted boxplots of quantity, unit price, and amount revealing extreme outliers. I decided to log transform these features for a better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'] = df['Quantity'] * df['UnitPrice']\n",
    "print(\"Summary Statistics (Quantity, UnitPrice, Amount):\")\n",
    "print(df[['Quantity', 'UnitPrice', 'Amount']].describe())\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "sns.boxplot(y=df['Quantity'], ax=axes[0]).set_title('Quantity')\n",
    "sns.boxplot(y=df['UnitPrice'], ax=axes[1]).set_title('UnitPrice')\n",
    "sns.boxplot(y=df['Amount'], ax=axes[2]).set_title('Amount')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# remove extreme outliers\n",
    "filtered_df = df.loc[\n",
    "    (df['Quantity'] > -1000) & (df['Quantity'] < 1000) &\n",
    "    (df['UnitPrice'] > 0) & (df['UnitPrice'] < 1000)].copy() # create a copy here to aovid warnings\n",
    "\n",
    "# log-transformations\n",
    "filtered_df['Log_Quantity'] = np.log1p(np.abs(filtered_df['Quantity']))\n",
    "filtered_df['Log_UnitPrice'] = np.log1p(filtered_df['UnitPrice'])\n",
    "\n",
    "# calc the amount\n",
    "filtered_df['Amount'] = filtered_df['Quantity'] * filtered_df['UnitPrice']\n",
    "\n",
    "# filter based on Amount\n",
    "filtered_df = filtered_df.loc[(filtered_df['Amount'] > -1000) & (filtered_df['Amount'] < 1000)].copy() # create a copy again\n",
    "\n",
    "# log-transform the Amount\n",
    "filtered_df['Log_Amount'] = np.log1p(np.abs(filtered_df['Amount']))\n",
    "\n",
    "# replot the boxplots with log-transformation\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "sns.boxplot(y=filtered_df['Log_Quantity'], ax=axes[0]).set_title('Log_Quantity')\n",
    "sns.boxplot(y=filtered_df['Log_UnitPrice'], ax=axes[1]).set_title('Log_UnitPrice')\n",
    "sns.boxplot(y=filtered_df['Log_Amount'], ax=axes[2]).set_title('Log_Amount')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# make subplots for filtered and transformed data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# histogram for log-transformed Quantity\n",
    "sns.histplot(filtered_df['Log_Quantity'], bins=50, kde=True, ax=axes[0])\n",
    "axes[0].set_title('Log-Transformed Distribution of Quantity')\n",
    "axes[0].set_xlabel('Log(1 + |Quantity|)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# histogram for log-transformed UnitPrice\n",
    "sns.histplot(filtered_df['Log_UnitPrice'], bins=50, kde=True, ax=axes[1])\n",
    "axes[1].set_title('Log-Transformed Distribution of Unit Price')\n",
    "axes[1].set_xlabel('Log(1 + Unit Price)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# histogram for log-transformed Amount\n",
    "sns.histplot(filtered_df['Log_Amount'], bins=50, kde=True, ax=axes[2])\n",
    "axes[2].set_title('Log-Transformed Distribution of Amount')\n",
    "axes[2].set_xlabel('Log(1 + |Amount|)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0U8bED3jlZIv"
   },
   "source": [
    "## Insights\n",
    "**Quantity:**\n",
    " - Mean (13): The average quantity per transaction is relatively small.\n",
    " - Standard Deviation (180): The very high stdev means there is significant variation in the quantity purchased across transactions.\n",
    " - Max (80995): This high quantity could be from bulk purchases or data errors.\n",
    " - 25th Percentile (2): Many transactions involve single item purchases.\n",
    " - Median (6): Half of the transactions involve 6 or fewer items.\n",
    " - 75th Percentile (12): 75% of transactions involve fewer than 12 items.\n",
    "\n",
    "**UnitPrice:**\n",
    " - Mean (3): The average price per unit is reasonable but may also be skewed by extreme values.\n",
    " - Standard Deviation (22): This moderately high standard deviation means there is price variation across products, which makes sense for this large dataset.\n",
    " - Min (0): likely correspond to promotional items, this needs to be examined to decide whether to include these items in the clustering algorithms.\n",
    " - Max (8143): High prices could represent premium items, but they should be investigated.\n",
    " - 25th Percentile (1.25): Most items are low cost products.\n",
    " - Median (1.95): Typical items are inexpensive.\n",
    " - 75th Percentile (3.75): 75% of items are below $3.75.\n",
    "\n",
    "**Amount**\n",
    "- Mean (22): The average is higher than the 50th percentile (12), indicating a right skewed distribution (a few high value transactions bring the average up).\n",
    "- Standard Deviation (309): This high standard deviation suggests significant price and quantity variation across products.\n",
    "- Max (168,470): likely correspond to data entry errors or large corporate purchases.\n",
    "- Min (0): likely correspond to promotional items, this needs to be examined to decide whether to include these items in the clustering algorithms.\n",
    "- 25th percentile (5): suggests that most transactions are of small amounts.\n",
    "- 75th percentile (20): Suggested that a few very large transactions drive the outlier values.\n",
    "\n",
    "**Boxplots:**\n",
    "There are significant outliers in Quantity, Unit Price, and Amount. Log-transformed plots also reveal many outliers. These outliers might skew clustering and need careful handling.\n",
    "\n",
    "**Histograms (bottom row):**\n",
    "The log-transformed distributions are more normalized, especially for Unit Price and Amount, which supports the use of normalization for clustering or modeling. Quantity remains slightly skewed and will need to be explored more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I82kOZCo58F"
   },
   "source": [
    "## Zero-Priced Items Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for anomalies in UnitPrice\n",
    "zero_price = df[df['UnitPrice'] <= 0]\n",
    "print(f\"Number of Transactions with Zero or Negative Unit Price: {len(zero_price)}\")\n",
    "print(zero_price[['InvoiceNo', 'Quantity', 'Description', 'UnitPrice']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THtXGZYVpIO9"
   },
   "source": [
    "## Zero Unit Prices\n",
    "There are 40 transactions with zero unit prices. These entries might be from free promotional items, data entry errors, or missing pricing data. So, I will explore whether these zero-priced items are promotional products or data errors. If they are invalid or irrelevant, I will remove these rows. Otherwise, I will categorize them as promotional items for analysis. I will check if zero prices align with specific dates like Black Friday, holiday seasons, or promotions. Also I will see if certain countries have more zero-priced transactions. Lastly, I will check if specific StockCode values correlate with zero-priced items (marketing giveaways or invalid codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_price_by_date = df[df['UnitPrice'] == 0].groupby('InvoiceDate').size()\n",
    "print(zero_price_by_date)\n",
    "\n",
    "zero_price_by_country = df[df['UnitPrice'] == 0].groupby('Country').size()\n",
    "print(zero_price_by_country)\n",
    "\n",
    "zero_price_by_stockcode = (df[df['UnitPrice'] == 0][['StockCode', 'Description']].value_counts().sort_values(ascending=False).head())\n",
    "print(zero_price_by_stockcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVzeGR5SpX5m"
   },
   "source": [
    "Zero-priced items by dates reveal no patterns. Zero-priced items by country do not reveal any clear patterns, esepcially since the dataset is majority UK purchases. Certain StockCode values (M) appear frequently with zero prices. This code is for Manuals. Since there are only 40 zero-priced items, removing them will not negatively impact my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zero-priced items from the main dataset to prevent skewed clusters\n",
    "df = df[df['UnitPrice'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xZLMNwkDTf7"
   },
   "source": [
    "Checking for the most and least popular items can inform the business which products they should focus or ignore marketing for. I decdied to find the most and least popular items by their total quantity sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 most popular items\n",
    "popular_items = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "popular_items.plot(kind='bar', figsize=(10, 6), title='Top 10 Most Popular Items by Quantity')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.show()\n",
    "\n",
    "# top 10 least popular items (excluding negatives/returns)\n",
    "least_popular_items = df.groupby('Description')['Quantity'].sum().sort_values(ascending=True).head(10)\n",
    "# bar plot for least popular items\n",
    "least_popular_items.plot(kind='bar', figsize=(10, 6), title='Top 10 Least Popular Items by Quantity')\n",
    "plt.ylabel('Total Quantity Sold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q85p4Ik7nFQV"
   },
   "source": [
    "## Time-based/Seasonal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly sales data\n",
    "monthly_sales = df.groupby('Month', observed=False)['Amount'].sum().sort_values(ascending=False)\n",
    "# day-of-week sales data\n",
    "weekday_sales = df.groupby('DayOfWeek', observed=False)['Amount'].sum()\n",
    "\n",
    "# make subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))  # 1 row, 2 columns\n",
    "# monthly sales plot\n",
    "monthly_sales.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Sales by Month')\n",
    "axes[0].set_ylabel('Total Sales')\n",
    "axes[0].set_xlabel('Month')\n",
    "# day-of-week sales plot\n",
    "weekday_sales.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Sales by Day of the Week')\n",
    "axes[1].set_ylabel('Total Sales')\n",
    "axes[1].set_xlabel('Day of the Week')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to holiday days\n",
    "holiday_df = df[df['HolidayPeriod'] == 'Holiday Season'].copy()\n",
    "\n",
    "# since 2 years, do Month+Year: dt.to_period('M') changes InvoiceDate col to monthly period (2011-12-15 --> 2011-12)\n",
    "holiday_df['MonthYear'] = holiday_df['InvoiceDate'].dt.to_period('M').astype(str)\n",
    "holiday_pivot = holiday_df.pivot_table(values='Amount', index='DayOfWeek', columns='MonthYear', aggfunc='sum', observed=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(holiday_pivot, cmap='RdBu', annot=True, fmt='.0f')\n",
    "plt.title('Holiday Period Heatmap: DayOfWeek vs. Month-Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaXHfBGj-m9u"
   },
   "source": [
    "## Insights from the Time-Based Analysis:\n",
    "**Monthly Sales:** November and December have the highest sales, likely due to the holiday season. There are low sales in February and April. These months might be non peak season, with fewer shopping events or promotions.\n",
    "\n",
    "**Day of the Week:**\n",
    "\n",
    "Thursday and Tuesday have the highest sales. This could just be regular midweek shopping. Sunday is the lowest, likely due to store closures or reduced hours in certain areas.\n",
    "\n",
    "**Holiday Period Heatmap:**\n",
    "\n",
    "Sales are highest on Fridays during the holiday season, likely due to holiday shopping and gift purchases after the work week. Sundays consistently show the least activity across both December 2010 and December 2011. Sales on Thursdays and Tuesdays also show strong performance during December. This means that the holiday season, particularly Fridays, is a critical period for sales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkFJuqDFz-zQ"
   },
   "source": [
    "## Additional Steps Before Clustering\n",
    "\n",
    "Given the high holiday-season volume, I decided to focus on holiday shoppers. First, I built aggregated features per customers in that holiday window for clustering. I then added an average holiday amount feature in order to give insights on how customers spend on each transaction. This will improve clustering quality by giving an additional dimensions to segregate customers by. Additionally, it will help balance the feature significance since certain customers may have a very high total amount spent.\n",
    "\n",
    "In the following cell, I remove outliers to reduce their influence. Since outliers were signifcantly skewing my clustering results due to the distance based nature of k-means and hierarchial clustering techbnniques, replacing extreme values with these threholds will make sure the clusters are based on typical customer spending habits. Capping the extreme values makes the clusters more representative of the overall population. This will allow me to make more meaningful groupings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset only holiday purchases (including Black Friday and December holiday season)\n",
    "holiday_only_df = df[df['HolidayPeriod'].isin(['Black Friday', 'Holiday Season'])].copy()\n",
    "\n",
    "# aggregate features at the customer level (only holiday rows)\n",
    "customer_holiday_data = holiday_only_df.groupby('CustomerID').agg({\n",
    "    'Amount': 'sum', # total holiday spending\n",
    "    'Quantity': 'sum', # total holiday quantity\n",
    "    'InvoiceNo': 'nunique' # number of holiday invoices\n",
    "}).reset_index()\n",
    "\n",
    "# rename columns for clarity\n",
    "customer_holiday_data.columns = ['CustomerID', 'HolidayTotalAmount', 'HolidayTotalQuantity', 'HolidayFrequency']\n",
    "\n",
    "# make an additional AvgHolidayAmount feature\n",
    "customer_holiday_data['AvgHolidayAmount'] = (customer_holiday_data['HolidayTotalAmount'] / customer_holiday_data['HolidayFrequency'])\n",
    "print(\"Holiday only customer level data:\")\n",
    "print(customer_holiday_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_thresholds(dataframe, variable, q1=0.03, q3=0.97):\n",
    "    quartile1 = dataframe[variable].quantile(q1)\n",
    "    quartile3 = dataframe[variable].quantile(q3)\n",
    "    interquantile_range = quartile3 - quartile1\n",
    "    up_limit = quartile3 + 1.5 * interquantile_range\n",
    "    low_limit = quartile1 - 1.5 * interquantile_range\n",
    "    return up_limit, low_limit\n",
    "\n",
    "def replace_with_threshold(dataframe, variable):\n",
    "    up_limit, low_limit = outlier_thresholds(dataframe, variable)\n",
    "    dataframe[variable] = dataframe[variable].astype(float)\n",
    "    dataframe.loc[dataframe[variable] > up_limit, variable] = up_limit\n",
    "    dataframe.loc[dataframe[variable] < low_limit, variable] = low_limit\n",
    "\n",
    "outlier_cols = ['HolidayTotalAmount', 'HolidayTotalQuantity', 'HolidayFrequency', 'AvgHolidayAmount']\n",
    "for col in outlier_cols:\n",
    "    replace_with_threshold(customer_holiday_data, col)\n",
    "\n",
    "print(\"Data after outlier removal:\")\n",
    "print(customer_holiday_data[outlier_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cjw31SEE8jh"
   },
   "source": [
    "Here, I scale all the features before clustering to make sure that all features contribute equally to the process, avoiding bias from features with larger numerical ranges. The Min-Max scaler is a good option because it normalizes data to 0 to 1, preserving relative distances between points. In agglomerative clustering using the cosine metric, the similarity is based on the angle between vectors, not just the magnitude. Zero-row vectors (rows with all features equal to zero) cause issues because cosine similarity is undefined for zero vectors, as the dot product with any vector is zero, and division by zero occurs. Filtering out zero-priced items beforehand leaves zero-row vectors, which need to be excluded to avoid errors. Removing these rows makes sure the clustering algorithm functions without numerical instability errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_features = ['HolidayTotalAmount', 'HolidayTotalQuantity', 'HolidayFrequency', 'AvgHolidayAmount']\n",
    "\n",
    "X = customer_holiday_data[holiday_features].copy()\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# identify zero vector rows\n",
    "zero_vector_rows = (X_scaled == 0).all(axis=1)\n",
    "# filter out zero vectors\n",
    "X_scaled_no_zeros = X_scaled[~zero_vector_rows]\n",
    "customer_holiday_data= customer_holiday_data.loc[~zero_vector_rows].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WexwZyMiwH3b"
   },
   "source": [
    "# 3. Model Building and Analysis\n",
    "\n",
    "Below, I evaluate agglomerative clustering with multiple linkage methods (ward, complete, average, single) and distance metrics (euclidean, manhattan, cosine). I fix n_clusters=3 to group holiday shoppers into the three following groups: non-holiday shoppers, moderate holiday shoppers, and frequent holiday shoppers.\n",
    "\n",
    "\n",
    "evaluate_clustering: Calculates the three key metrics: silhouette score (higher = better separation), Calinski-Harabasz index (higher = better), Davies-Bouldin score (lower = better).\n",
    "\n",
    "Iterate Over Linkage & Distance: I test all combinations of linkage (complete, average, single, ward) and distance metrics (euclidean, manhattan, cosine) to find the best silhouette score.\n",
    "\n",
    "Best Model: The model with the highest silhouette score is saved. I then plot it in 2D PCA space for a visual of how the clusters are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(labels, data):\n",
    "    silhouette = silhouette_score(data, labels)\n",
    "    calinski = calinski_harabasz_score(data, labels)\n",
    "    davies = davies_bouldin_score(data, labels)\n",
    "    return silhouette, calinski, davies\n",
    "\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "distance_metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "\n",
    "best_hierarchical_score = -1\n",
    "best_hier_model = None\n",
    "best_hier_labels = None\n",
    "best_linkage = None\n",
    "best_metric = None\n",
    "\n",
    "for linkage in linkage_methods:\n",
    "    for metric in distance_metrics:\n",
    "        # ward linkage only supports euclidean distance\n",
    "        if linkage == 'ward' and metric != 'euclidean':\n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "        # fix n_clusters=3\n",
    "        model = AgglomerativeClustering(n_clusters=3, linkage=linkage, metric=metric)\n",
    "        labels = model.fit_predict(X_scaled_no_zeros)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        silhouette, calinski, davies = evaluate_clustering(labels, X_scaled_no_zeros)\n",
    "        print(f\"Agglomerative ({linkage}, {metric}) \"\n",
    "              f\"- Silhouette: {silhouette:.3f}, \"\n",
    "              f\"Calinski-Harabasz: {calinski:.3f}, \"\n",
    "              f\"Davies-Bouldin: {davies:.3f}, \"\n",
    "              f\"Time: {elapsed:.3f}s\")\n",
    "\n",
    "        # track best by silhouette\n",
    "        if silhouette > best_hierarchical_score:\n",
    "            best_hierarchical_score = silhouette\n",
    "            best_hier_model = model\n",
    "            best_hier_labels = labels\n",
    "            best_linkage = linkage\n",
    "            best_metric = metric\n",
    "\n",
    "print(\"\\nBest Hierarchical Clustering Model:\")\n",
    "print(f\"- Linkage: {best_linkage}\")\n",
    "print(f\"- Distance Metric: {best_metric}\")\n",
    "print(f\"- Silhouette Score: {best_hierarchical_score:.3f}\")\n",
    "# store these best labels in customer_holiday_data\n",
    "customer_holiday_data['HierCluster'] = best_hier_labels\n",
    "print(\"\\nHierarchical Clustering Cluster Counts:\")\n",
    "print(customer_holiday_data['HierCluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled_no_zeros)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_hier_labels, cmap='plasma', alpha=0.7)\n",
    "plt.title(f\"Best Hierarchical Clusters (Linkage={best_linkage}, Metric={best_metric})\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U_Ezfr10IBK"
   },
   "source": [
    "**K-Means Clustering**\n",
    "\n",
    "Next, I compare K-Means with the same number of clusters (k=3) to see whether it forms meaningful segments for our holiday shoppers.\n",
    "\n",
    "Silhouette Score: Measures how distinct each cluster is.\n",
    "Calinski-Harabasz: Rewards clusters that are compact and well-separated.\n",
    "Davies-Bouldin: Penalizes clusters that overlap or are too close. PCA Visualization: I plot the data into 2D principal components to visualize the clusters. The red “X” marks are the cluster centroids in PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled_no_zeros)\n",
    "end_time = time.time()\n",
    "\n",
    "# store labels in your customer data\n",
    "customer_holiday_data['HolidayCluster'] = cluster_labels\n",
    "print(\"\\nK-means Cluster counts:\")\n",
    "print(customer_holiday_data['HolidayCluster'].value_counts())\n",
    "\n",
    "# calc silhouette score\n",
    "silhouette = silhouette_score(X_scaled_no_zeros, cluster_labels)\n",
    "print(f\"\\nK-means Silhouette Score for k=3: {silhouette:.3f}\")\n",
    "# calc Calinski-Harabasz Index\n",
    "calinski_harabasz = calinski_harabasz_score(X_scaled_no_zeros, cluster_labels)\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.3f}\")\n",
    "# calc Davies-Bouldin Score\n",
    "davies_bouldin = davies_bouldin_score(X_scaled_no_zeros, cluster_labels)\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin:.3f}\")\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Clustering Execution Time: {execution_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to 2 principal components for 2D plot\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled_no_zeros)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plot each sample in 2D PCA space, colored by cluster label\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "\n",
    "# plot the cluster centroids (in PCA space)\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids_pca = pca.transform(centroids)\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
    "\n",
    "plt.title(\"K-Means Clusters (PCA 2D)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpG3VKhzA6sY"
   },
   "source": [
    "## **Clustering Insights**\n",
    "**Metric Comparisons**\n",
    "\n",
    " - Silhouette Score:\n",
    "   - Hierarchical, average, manhattan might yield a higher silhouette (0.791) vs. K-Means (0.621), suggesting more distinct or irregularly shaped clusters that K-Means doesn't capture well.\n",
    "\n",
    " - Calinski-Harabasz Index (higher = better separation):\n",
    "   - K-Means does better here, scoring higher (1893) vs. hierarchical (664).\n",
    "\n",
    " - Davies-Bouldin Score (lower = better):\n",
    "   - Hierarchial, average, manhattan slightly outperforms hierarchical (0.884 vs. 0.577).\n",
    "\n",
    "**Why Hierarchical May Appear “Better”**\n",
    "\n",
    "1. Flexible cluster shapes: Hierarchical clustering doesn't assume spherical distributions, allowing it to adapt to complex structures.\n",
    "2. More sensitive to small differences: Linkage methods like complete or average isolate small and dense subgroups.\n",
    "\n",
    "**Why K-Means Could Still Be Preferable**\n",
    "1. Balanced clusters: K-Means distributes points more evenly, whereas hierarchical produces tiny clusters (a cluster with just 5 customers).\n",
    "2. Efficiency & Speed: K-Means is faster than hierarchical for large datasets.\n",
    "3. Frequent itemset mining: Since I need relatively balanced clusters for frequent itemset mining, K-Means yields more meaningful groups. A tiny cluster from hierarchical lacks enough transactions to discover frequent patterns.\n",
    "\n",
    "**Final Decision for Holiday Segmentation**\n",
    "\n",
    "Given that I want balanced segments for holiday shopper analysis and itemset mining, **K-Means** is a practical choice despite a lower silhouette score. It ensures each cluster has enough transactions to give me meaningful association rules and targeted marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_stats = customer_holiday_data.groupby('HolidayCluster')[['HolidayTotalAmount', 'HolidayFrequency']].mean()\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_label_map = {0: \"Non-Holiday Shoppers\", 1: \"Frequent Holiday Shoppers\", 2: \"Moderate Holiday Shoppers\"}\n",
    "customer_holiday_data['HolidayClusterName'] = (customer_holiday_data['HolidayCluster'].map(cluster_label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve2O0ycQsELD"
   },
   "source": [
    "## **Frequent Itemset Mining**\n",
    "**Key Terminology**\n",
    " - Antecedent (“IF”): A set of items that appear in a transaction together.\n",
    "    - In “IF {bread, milk} THEN {butter},” the antecedent is {bread, milk}.\n",
    " - Consequent (“THEN”): A set of items predicted to appear given the antecedent.\n",
    "  - In “IF {bread, milk} THEN {butter},” the consequent is {butter}.\n",
    " - Support: Probability of transactions containing both antecedent and consequent. Typical thresholds are 1 to 10%.\n",
    " - Confidence: Probability of the consequent given the antecedent. Typically, we want between 50 to 100%.\n",
    " - Lift: Measures how many times more likely the consequent is to occur with the antecedent than by chance alone. Values >1 indicate a positive correlation.\n",
    "\n",
    "Here, I use FPGrowth (Frequent Pattern Growth) to efficiently find itemsets with at least a certain min_support. Then, association_rules is applied to generate rules that meet a min_threshold for confidence.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "1. Cluster Subset: I only look at transactions for the customers in each cluster.\n",
    "2. Basket Construction:\n",
    " - Rows = invoices.\n",
    " - Columns = product descriptions.\n",
    " - Cells = 1 (True) if the invoice contains the item, 0 (False) if not.\n",
    "3. Pruning Rare Items: Items that appear in fewer than 10 invoices are removed to reduce noise and memory usage.\n",
    "4. FPGrowth: Finds sets of items that co-occur in transactions with min_support >= 0.05.\n",
    "5. Association Rules: For each frequent itemset, generate rules that meet a minimum confidence (20%).\n",
    "\n",
    "Result: A rules_sorted df containing antecedents, consequents, support, confidence, and lift. Higher confidence and lift indicate stronger co-purchase patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_rules = {}\n",
    "for cluster_id in sorted(customer_holiday_data['HolidayCluster'].unique()):\n",
    "    # fetch the string name from the map\n",
    "    cluster_name = cluster_label_map.get(cluster_id, f\"Unknown Cluster {cluster_id}\")\n",
    "    print(f\"\\nCLUSTER {cluster_id} ({cluster_name}):\")\n",
    "\n",
    "    # 1. subset customers\n",
    "    cluster_customers = customer_holiday_data.loc[customer_holiday_data['HolidayCluster'] == cluster_id, 'CustomerID']\n",
    "\n",
    "    # 2. subset holiday transactions for these cluster customers\n",
    "    cluster_holiday_df = holiday_only_df[holiday_only_df['CustomerID'].isin(cluster_customers)].copy()\n",
    "\n",
    "    if cluster_holiday_df.empty:\n",
    "        print(f\"No transactions for cluster {cluster_id}\")\n",
    "        continue\n",
    "\n",
    "    # 3. build the basket\n",
    "    basket = (\n",
    "        cluster_holiday_df\n",
    "        .groupby(['InvoiceNo', 'Description_mode'])['Quantity']\n",
    "        .sum().unstack().fillna(0)\n",
    "        .applymap(lambda x: 1 if x > 0 else 0)\n",
    "        .astype(bool))\n",
    "\n",
    "    # prune rare items: remove columns with fewer than 10 occurrences\n",
    "    item_counts = basket.sum(axis=0) # sum across transactions (rows)\n",
    "    frequent_items = item_counts[item_counts >= 10].index\n",
    "    basket = basket[frequent_items]\n",
    "\n",
    "    print(\"Basket shape after pruning rare items:\", basket.shape)\n",
    "    if basket.shape[1] == 0:\n",
    "        print(f\"All items too rare for cluster {cluster_id}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 4. use FPGrowth (for better memory management) or Apriori\n",
    "    min_support = 0.05\n",
    "    frequent_itemsets = fpgrowth(basket, min_support=min_support, use_colnames=True)\n",
    "\n",
    "    if frequent_itemsets.empty:\n",
    "        print(f\"No frequent itemsets for cluster {cluster_id} at min_support={min_support}.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Found {len(frequent_itemsets)} frequent itemsets.\")\n",
    "\n",
    "    # 5. association rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.2, num_itemsets=2)\n",
    "\n",
    "    if rules.empty:\n",
    "        print(f\"No rules for cluster {cluster_id}.\")\n",
    "        continue\n",
    "\n",
    "    rules_sorted = rules.sort_values(by='confidence', ascending=False)\n",
    "    cluster_rules[cluster_id] = rules_sorted\n",
    "    print(rules_sorted[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEsfhMV9s2Gz"
   },
   "source": [
    "## Simple Rule-Based Recommender System\n",
    "Overview:\n",
    "\n",
    "Next, I make a simple “if-then” recommender system based on the discovered association rules. This takes the customer's current items as input, filters rules to those whose antecedents are subsets of the customer's items, collects consequents (“then” items) as recommendations, and ranks them by confidence and returns the top N suggestions.\n",
    "\n",
    "Logic:\n",
    "\n",
    "1. Current Items: Suppose the user already has {“PAPER CHAIN KIT VINTAGE CHRISTMAS”}.\n",
    "2. Match Antecedents: If your rule is (“PAPER CHAIN KIT VINTAGE CHRISTMAS”) -> (“PAPER CHAIN KIT 50'S CHRISTMAS”), and the customer's item set includes the antecedent, the system recommends the consequent.\n",
    "3. Confidence Ranking: I assume items from higher confidence rules are more relevant to the user.\n",
    "4. Final Recommendation List: I deduplicate and return up to top_n items.\n",
    "\n",
    "This is useful because a user who has “PAPER CHAIN KIT VINTAGE CHRISTMAS” might also want “PAPER CHAIN KIT 50'S CHRISTMAS.” This approach is clear and good for market basket style suggestions. It essentially automates looking through each frequent itemset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(current_items, rules_sorted, top_n=20):\n",
    "    # filter rules where antecedents is a subset of current_items\n",
    "    filtered_rules = rules_sorted[rules_sorted['antecedents'].apply(lambda ante: ante.issubset(current_items))]\n",
    "    # sort by highest confidence or lift (already sorted by conf in rules_sorted)\n",
    "    filtered_rules = filtered_rules.sort_values('confidence', ascending=False)\n",
    "\n",
    "    recommended = []\n",
    "    for _, row in filtered_rules.iterrows():\n",
    "        for item in row['consequents']:\n",
    "            if item not in current_items:\n",
    "                recommended.append(item)\n",
    "\n",
    "    # remove duplicates while preserving order\n",
    "    recommended = list(dict.fromkeys(recommended))\n",
    "    return recommended[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage: suppose a user from cluster 0 has these items:\n",
    "user_items = {\"PAPER CHAIN KIT VINTAGE CHRISTMAS\"}\n",
    "\n",
    "rules_cluster_0 = cluster_rules.get(0, pd.DataFrame())\n",
    "if not rules_cluster_0.empty:\n",
    "    recs = recommend_items(user_items, rules_cluster_0, top_n=20)\n",
    "    print(\"\\nRecommendations for a user with items:\", user_items)\n",
    "    print(recs)\n",
    "\n",
    "# example usage: suppose a user from cluster 1 has these items:\n",
    "user_items = {\"HAND WARMER RED RETROSPOT\", \"KNITTED UNION FLAG HOT WATER BOTTLE\"}\n",
    "\n",
    "rules_cluster_1 = cluster_rules.get(1, pd.DataFrame())\n",
    "if not rules_cluster_1.empty:\n",
    "    recs = recommend_items(user_items, rules_cluster_1, top_n=10)\n",
    "    print(\"\\nRecommendations for a user with items:\", user_items)\n",
    "    print(recs)\n",
    "\n",
    "# example usage: suppose a user from cluster 2 has these items:\n",
    "user_items = {\"ALARM CLOCK BAKELIKE GREEN\"}\n",
    "\n",
    "rules_cluster_2 = cluster_rules.get(2, pd.DataFrame())\n",
    "if not rules_cluster_2.empty:\n",
    "    recs = recommend_items(user_items, rules_cluster_2, top_n=20)\n",
    "    print(\"\\nRecommendations for a user with items:\", user_items)\n",
    "    print(recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmf9atI8wSfn"
   },
   "source": [
    "## Implicit Collaborative Filtering Recommender System\n",
    "\n",
    "In many real-world scenarios, we might not have users explicitly rating products with 1 to 5 stars. Instead, we only see which items they purchased or didn't purchase. This binary (1 or 0) is the implicit feedback. If a user bought an item, it suggests some level of interest and if not, it means no preference.\n",
    "\n",
    "Traditional collaborative filtering (CF) often relies on explicit ratings to calculate user-item similarities or perform matrix factorization. However, libraries like implicit handle these binary purchase matrices, allowing me to build a latent factor model without ratings.\n",
    "\n",
    "This was included because implicit collaborative filtering can adapt to new user preferences and recommend entirely new items, while the rule-based approach might require updates to the rules if new products are introduced. Here is how it works:\n",
    "\n",
    "1. User-Item Matrix (rows = users, columns = items):\n",
    "\n",
    " - In this code, each row corresponds to a CustomerID, and each column to a product description, with cell values set to 1 if the user purchased that product and 0 if not.\n",
    " - This matrix can be large, so I convert it to a sparse format (csr_matrix).\n",
    "\n",
    "2. Alternating Least Squares (ALS) from implicit:\n",
    "\n",
    " - ALS factorizes the user-item matrix into latent factors, representing user preferences and item attributes.\n",
    " - It uses implicit feedback (purchase history) and confidence weighting to model interactions effectively.\n",
    " - Recall@N is used to evaluate the recommendations, as it measures how well the model captures relevant items among the top-N recommendations. This is a critical metric for recommender systems prioritizing relevance.\n",
    "\n",
    " 3. Hyperparameter Tuning and Validation\n",
    "\n",
    " - The data is split into training and validation sets using train_val_split.\n",
    " - Optuna is used to tune ALS hyperparameters like factors, regularization,  and iterations to maximize recall@N on the validation set.\n",
    "\n",
    "\n",
    "4. Cluster Integration:\n",
    "\n",
    " - I do this for each holiday cluster, building a cluster-specific user-item matrix. This ensures recommendations are tailored to the patterns within that cluster.\n",
    " - Each cluster's ALS model is trained using the best hyperparameters found during tuning, ensuring optimal performance across different customer segments.\n",
    "\n",
    " 5. Recommendations:\n",
    "\n",
    "- For demonstration, the model generates  recommendations for a sample customer within each cluster, listing their past purchases and suggesting new items with relevance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCctM282oROS"
   },
   "source": [
    "## Hyperparameter Tuning with Recall@N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(user_item_csr, val_ratio=0.2):\n",
    "    # 1. convert csr matrix to coo format for easy iteration\n",
    "    coo = user_item_csr.tocoo()\n",
    "    interactions = list(zip(coo.row, coo.col))\n",
    "    np.random.shuffle(interactions)\n",
    "\n",
    "    # 2. split into train and validation interactions\n",
    "    val_size = int(len(interactions) * val_ratio)\n",
    "    val_interactions = interactions[:val_size]\n",
    "    train_interactions = interactions[val_size:]\n",
    "\n",
    "    # 3. create train and validation csr matrices\n",
    "    train_row, train_col = zip(*train_interactions)\n",
    "    val_row, val_col = zip(*val_interactions)\n",
    "\n",
    "    train_csr = csr_matrix((np.ones(len(train_row)), (train_row, train_col)), shape=user_item_csr.shape)\n",
    "    val_csr = csr_matrix((np.ones(len(val_row)), (val_row, val_col)), shape=user_item_csr.shape)\n",
    "    return train_csr, val_csr\n",
    "\n",
    "def recall_at_n(model, train_csr, val_csr, N=5):\n",
    "    # 1. convert validation set to coo format and get all (user, item) pairs\n",
    "    val_coo = val_csr.tocoo()\n",
    "    val_positives = list(zip(val_coo.row, val_coo.col))\n",
    "    total_recall = 0\n",
    "    num_users = 0\n",
    "\n",
    "    for user, _ in val_positives:\n",
    "        # 2. skip invalid users if necessary\n",
    "        if user >= train_csr.shape[0]:\n",
    "            continue\n",
    "        num_users += 1\n",
    "\n",
    "        # 3. get top-n recommendations for the user\n",
    "        recommended = model.recommend(userid=user, user_items=train_csr[user], N=N, filter_already_liked_items=False)\n",
    "        recommended_items = set(recommended[0])  # extract item indices\n",
    "\n",
    "        # 4. count how many true items are in the top-n recommendations\n",
    "        true_items = set(val_csr[user].nonzero()[1])\n",
    "        total_recall += len(recommended_items & true_items) / len(true_items)\n",
    "\n",
    "    # 5. return average recall across all users\n",
    "    return total_recall / num_users if num_users > 0 else 0\n",
    "\n",
    "def objective(trial, user_item_csr, val_ratio=0.2):\n",
    "    # 1. suggest hyperparameters for the trial\n",
    "    n_factors = trial.suggest_int('factors', 18, 23)\n",
    "    reg = trial.suggest_float('regularization', 0.07, 0.12)\n",
    "    iters = trial.suggest_int('iterations', 10, 15)\n",
    "\n",
    "    # 2. split data into training and validation sets\n",
    "    train_csr, val_csr = train_val_split(user_item_csr, val_ratio=val_ratio)\n",
    "\n",
    "    # 3. train ALS model with the trial parameters\n",
    "    model = AlternatingLeastSquares(factors=n_factors, regularization=reg, iterations=iters, random_state=42)\n",
    "    model.fit(train_csr)\n",
    "\n",
    "    # 4. compute recall@n and return it as the trial's score\n",
    "    return recall_at_n(model, train_csr, val_csr, N=5)\n",
    "\n",
    "def tune_als_with_optuna(user_item_csr, n_trials=50):\n",
    "    # 1. create and optimize optuna study, limiting threads for the optimization process\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: objective(trial, user_item_csr), n_trials=n_trials)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kWhuerlFvjU"
   },
   "source": [
    "## Implicit Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. subset to just this cluster's holiday rows\n",
    "cluster_customers = customer_holiday_data.loc[customer_holiday_data['HolidayCluster'] == cluster_id, 'CustomerID']\n",
    "cluster_holiday_df = holiday_only_df[holiday_only_df['CustomerID'].isin(cluster_customers)].copy()\n",
    "\n",
    "# 2. build the user-item matrix (rows=CustomerID, columns=Description_mode, binary flags)\n",
    "user_item = (cluster_holiday_df.groupby(['CustomerID', 'Description_mode'])['Quantity'].sum().unstack(fill_value=0)\n",
    "             .applymap(lambda x: 1 if x > 0 else 0))  # binary flag\n",
    "user_item_csr = csr_matrix(user_item.values)\n",
    "\n",
    "# 3. hyperparameter tuning\n",
    "study = tune_als_with_optuna(user_item_csr, n_trials=50)\n",
    "best_params = study.best_params\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# 4. # customer_holiday_data has cluster labels and holiday_only_df is holiday transactions\n",
    "for cluster_id in sorted(customer_holiday_data['HolidayCluster'].unique()):\n",
    "    # 4.1 subset to this cluster's data\n",
    "    cluster_customers = customer_holiday_data.loc[customer_holiday_data['HolidayCluster'] == cluster_id, 'CustomerID']\n",
    "    cluster_holiday_df = holiday_only_df[holiday_only_df['CustomerID'].isin(cluster_customers)].copy()\n",
    "\n",
    "    # 4.2 build the cluster-specific user-item matrix\n",
    "    user_item = (cluster_holiday_df.groupby(['CustomerID', 'Description_mode'])['Quantity'].sum().unstack(fill_value=0)\n",
    "                 .applymap(lambda x: 1 if x > 0 else 0)) # binary flag to convert >0 --> 1\n",
    "    user_item_csr = csr_matrix(user_item.values)\n",
    "\n",
    "    # 4.3 train ALS model with the best hyperparameters\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        als_model = AlternatingLeastSquares(factors=best_params['factors'], \n",
    "                                            regularization=best_params['regularization'], \n",
    "                                            iterations=best_params['iterations'], \n",
    "                                            random_state=42, \n",
    "                                            num_threads=0)\n",
    "        als_model.fit(user_item_csr)\n",
    "\n",
    "    # 4.4 i'll pick the first CustomerID in this cluster as demonstration, but in real system you'd read from user input/API call\n",
    "    actual_cust_id = user_item.index[0]\n",
    "    # also, to use a specific known ID, like 12433:\n",
    "    #   if 12433 in user_item.index:\n",
    "    #       actual_cust_id = 12433\n",
    "    #   else skip\n",
    "    user_idx = user_item.index.get_loc(actual_cust_id)\n",
    "    user_items_row = user_item_csr.getrow(user_idx)\n",
    "\n",
    "    print(f\"\\nCLUSTER {cluster_id}:\")\n",
    "    print(f\"CustomerID={actual_cust_id} purchased:\")\n",
    "    purchased_mask = (user_item.iloc[user_idx] > 0)\n",
    "    purchased_items = user_item.columns[purchased_mask]\n",
    "    for desc in purchased_items:\n",
    "        print(f\"  - {desc}\")\n",
    "\n",
    "    # 4.5 generate top-n recommendations for the customer\n",
    "    recommended = als_model.recommend(userid=user_idx, user_items=user_items_row, N=5)\n",
    "    rec_indices, rec_scores = recommended\n",
    "    stock_codes = user_item.columns\n",
    "    print(f\"\\nRecommendations for CustomerID={actual_cust_id} in cluster {cluster_id}:\")\n",
    "    for idx, score in zip(rec_indices, rec_scores):\n",
    "        item_desc = stock_codes[idx]\n",
    "        print(f\"   {item_desc} (score={score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KzopNN_wPdb"
   },
   "source": [
    "# 4. Discussion and Conclusions\n",
    "\n",
    "This project shows a complete workflow from EDA and data cleaning to unsupervised clustering, frequent itemset mining, and both simple and implicit recommendation systems for holiday shoppers in an online retail dataset.\n",
    "\n",
    "1. Data Cleaning & Preparation\n",
    "\n",
    " - Addressing missing customer IDs and NaN values with removal as well as standardizing descriptions improve the accuracy of both clustering and recommender systems.\n",
    " - Deriving date features (day of week, holiday periods) revealed a large concentration of sales in November to December, justifying a holiday-focused analysis.\n",
    "2. Holiday Segmentation\n",
    "\n",
    " - K-Means vs. Hierarchical Clustering showed different cluster shapes and sizes.\n",
    " - K-Means delivered more balanced clusters, making it better for the following frequent itemset mining because small clusters can lack enough transactions for meaningful rules.\n",
    " - Hierarchical methods scored higher on silhouette, indicating the data may have irregular cluster shapes, but at the cost of highly imbalanced cluster sizes.\n",
    "3. Frequent Itemset Mining\n",
    "\n",
    " - After clustering, FPGrowth uncovered frequently co-purchased items within each holiday segment.\n",
    " - By pruning rare items and requiring a 5% support threshold, I focused on itemsets likely to have broad marketing impact. Outputs from FPGrowth revealed co-purchased itemsets within each cluster like \"HAND WARMER UNION JACK\" with \"HAND WARMER RED RETROSPOT\" in frequent holiday shoppers.\n",
    "- High confidence association rules identified opportunities for bundling and cross-selling complementary products. For example, suggesting \"PAPER CHAIN KIT 50'S CHRISTMAS\" for customers purchasing \"PAPER CHAIN KIT VINTAGE CHRISTMAS.\"\n",
    "4. Rule-Based Recommender\n",
    "\n",
    " - A simple “if-then” system uses association rules: if a user already bought the antecedent items, suggest the consequents.\n",
    " - While simple, it performs well in situations with clear, high-confidence patterns.\n",
    "5. Implicit Feedback Recommender\n",
    "\n",
    " - For a more robust solution, the ALS model from the implicit library handles large item spaces and captures latent patterns, even without explicit ratings.\n",
    " - Outputs included meaningful suggestions, such as \"ALARM CLOCK BAKELIKE IVORY\" for a customer with interest in other \"ALARM CLOCK BAKELIKE\" products.\n",
    " - Recall@N optimization ensured the model prioritized identifying relevant items among top-N recommendations, aligning with business goals for driving sales and engagement.\n",
    "\n",
    "\n",
    "**Key Takeaways**\n",
    "- Holiday Focus: By zeroing in on November to December, I discovered seasonal purchase behaviors, offering high impact marketing strategies for holiday seasons.\n",
    "- Clustering + Itemset Mining: Balanced clusters from K-Means ensured each segment had enough data for representative frequent itemsets, allowing more cluster-specific product recommendations. Non-holiday shoppers had fewer frequent itemsets (25) compared to frequent holiday shoppers (33) and moderate holiday shoppers (45). This alloiws a greater opportunity to target holiday segments with bundled promotions.\n",
    "- Recommendation Options:\n",
    "  1. Simple Rule-Based: Provided simple, actionable recommendations, like for frequent holiday shoppers, suggesting \"WHITE HANGING HEART T-LIGHT HOLDER\" based on prior purchases of \"KNITTED UNION FLAG HOT WATER BOTTLE.\"\n",
    "\n",
    "  2. ALS Implicit—can provide more scalable, personalized recommendations for cluster-specific needs, like offering \"SET OF 4 PANTRY JELLY MOULDS\" to a frequent holiday shopper with interest in kitchen items.\n",
    "\n",
    "**Business Implications**\n",
    "1. Targeted Promotions:\n",
    " - Use frequent itemsets to bundle high confidence complementary products. For example, pairing \"HAND WARMER UNION JACK\" with \"HAND WARMER RED RETROSPOT\" in targeted ads or product suggestions for frequent holiday shoppers.\n",
    " - Highlight high lift and confidence items in email campaigns.\n",
    "2. Personalized Marketing by Segment:\n",
    " - For frequent holiday shoppers, emphasize premium or exclusive holiday-themed product like \"WHITE HANGING HEART T-LIGHT HOLDER\".\n",
    " - For moderate holiday shoppers, use personalized ALS recommendations to suggest relevant items they may not discover on their own.\n",
    "3. Scalable Personalization:\n",
    " - Use ALS-based recommenders for personalized product suggestions, which can adapt to customer behaviors across clusters and beyond the holiday season.\n",
    " - Use recall@N evaluation to ensure recommendations prioritize highly relevant products.\n",
    "\n",
    "In conclusions, by combining segmentation, frequent itemset mining, and hybrid recommendation approaches, this project provided actionable insights for tailoring retail strategies. Businesses can balance simplicity and interpretability (rule-based recommenders) with scalability and personalization (ALS) to drive customer engagement and sales, particularly during high-impact shopping seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9Lxi6GOgXsF"
   },
   "source": [
    "# References\n",
    "Chen, D. (2015). Online Retail Dataset. UCI Machine Learning Repository. https://doi.org/10.24432/C5BW33.\n",
    "\n",
    "Frederickson, B. (2025). Implicit: Fast Python Collaborative Filtering for Implicit Datasets (Version 0.7.2) [Computer software]. GitHub. https://github.com/benfred/implicit.\n",
    "\n",
    "Python Software Foundation. (2025). difflib — Helpers for Computing Deltas. In Python 3.13.1 Documentation. Retrieved from https://docs.python.org/3/library/difflib.html.\n",
    "\n",
    "Raschka, S. (2025). fpgrowth: Frequent Itemsets via the FP-Growth Algorithm. In mlxtend User Guide. Retrieved from https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/.\n",
    "\n",
    "\n",
    "Raschka, S. (2025). Apriori: Frequent Itemsets via the Apriori Algorithm. In mlxtend User Guide. Retrieved from https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
